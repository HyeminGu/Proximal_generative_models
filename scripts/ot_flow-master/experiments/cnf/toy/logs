/Users/hyemin/Documents/source_code/Proximal_generative_models/scripts/ot_flow-master/trainToyOTflow.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import plot4
from src.OTFlowProblem import *
import config
import pickle

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 1500
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings', 'student-t', 'Keystrokes'],
    type=str, default='student-t'
)
parser.add_argument(
    '-df', type=float, help="degree of freedom for student-t distribution"
)

parser.add_argument("--nt"    , type=int, default=16, help="number of time steps")
parser.add_argument("--nt_val", type=int, default=16, help="number of time steps for validation")
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0')
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)
parser.add_argument('--T'     , type=float, default=1.0)
parser.add_argument('--exclude_OT'  , type=bool, default=False)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)


parser.add_argument('--lr'          , type=float, default=0.1)
parser.add_argument("--drop_freq"   , type=int  , default=100, help="how often to decrease learning rate")
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='single', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=10000)#25)


args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]
if args.exclude_OT:
    args.alph[0] = 0.0
    args.alph[2] = 0.0

# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

torch.manual_seed(0)
device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def compute_loss(net, x, nt):
    Jc , cs = OTFlowProblem(x, net, [0,T], nt=nt, stepper="rk4", alph=net.alph)
    return Jc, cs


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    if args.data == 'Keystrokes':
        d = 1
    else:
        d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)
    T      = args.T

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None
    
    savedir = f"../../assets/{args.data}/"
    if not os.path.exists(savedir):
        os.makedirs(savedir)

    # setup data [nSamples, d]
    # use one batch as the entire data set
    if args.data == 'student-t':
        misc_params = {'df':args.df}
    else:
        misc_params = {}

    x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)
    x0 = cvt(torch.from_numpy(x0))

    x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, misc_params=misc_params)
    x0val = cvt(torch.from_numpy(x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s}      {:9s}  {:9s}  {:9s}  {:9s}  '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'valLoss', 'valL', 'valC', 'valR'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs  = compute_loss(net, x0, nt=nt)
        loss.backward()
        optim.step()

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e}  '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs = compute_loss(net, x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss   = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()
                

        logger.info(log_message) # print iteration

        # create plots
        if itr % args.viz_freq == 0:
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                nSamples = 20000
                p_samples = cvt(torch.Tensor( toy_data.inf_train_gen(args.data, batch_size=nSamples, misc_params=misc_params) ))
                y = cvt(torch.randn(nSamples,d)) # sampling from the standard normal (rho_1)

                '''
                if d == 2:
                    sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                    plot4(net, p_samples, y, [0,T], nt_val, sPath, doPaths=True, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                            ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh))
                '''
                # save result
                d = net.d
                fx = integrate(p_samples[:, 0:d], net, [0,T], nt_val, stepper="rk4", alph=net.alph)
                finvfx = integrate(fx[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
                genModel = integrate(y[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
    
                if args.exclude_OT == False:
                    filename = f"{savedir}otflow_"
                else:
                    filename = f"{savedir}no_otflow_"
                if args.df != None:
                    filename += f"df{args.df}_{args.batch_size}samples.pickle"
                else:
                    filename += f"{args.batch_size}samples.pickle"
                
                with open(filename,"wb") as fw:
                    pickle.dump([fx.cpu().numpy()[:, 0:d], genModel.cpu().numpy()[:, 0:d], ] , fw)
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
    logger.info(f"Result was stored in {filename}.")






start time: 2024_05_21_22_06_20
Namespace(data='student-t', df=1.0, nt=16, nt_val=16, alph=[1.0, 100.0, 5.0], m=32, nTh=2, T=10.0, exclude_OT=False, niters=20, batch_size=10000, val_batch_size=4096, lr=0.1, drop_freq=100, weight_decay=0.0, lr_drop=2.0, optim='adam', prec='single', save='experiments/cnf/toy', viz_freq=100, val_freq=1, gpu=0, sample_freq=10000)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=16   nt_val=16
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.1
    maximize: False
    weight_decay: 0.0
)
data=student-t batch_size=10000 gpu=0
maxIters=20 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
00001   1.286   3.774e+04  5.138e+03  6.311e+01  5.259e+03      6.843e+04  7.327e+03  2.356e+02  7.509e+03 
00002   1.246   4.755e+04  3.395e+03  2.625e+02  3.582e+03      5.248e+04  7.582e+03  5.479e+01  7.885e+03 
/Users/hyemin/Documents/source_code/Proximal_generative_models/scripts/ot_flow-master/trainToyOTflow.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import plot4
from src.OTFlowProblem import *
import config
import pickle

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 1500
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings', 'student-t', 'Keystrokes'],
    type=str, default='student-t'
)
parser.add_argument(
    '-df', type=float, help="degree of freedom for student-t distribution"
)

parser.add_argument("--nt"    , type=int, default=16, help="number of time steps")
parser.add_argument("--nt_val", type=int, default=16, help="number of time steps for validation")
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0')
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)
parser.add_argument('--T'     , type=float, default=1.0)
parser.add_argument('--exclude_OT'  , type=bool, default=False)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)


parser.add_argument('--lr'          , type=float, default=0.1)
parser.add_argument("--drop_freq"   , type=int  , default=100, help="how often to decrease learning rate")
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='single', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=10000)#25)


args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]
if args.exclude_OT:
    args.alph[0] = 0.0
    args.alph[2] = 0.0

# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

torch.manual_seed(0)
device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def compute_loss(net, x, nt):
    Jc , cs = OTFlowProblem(x, net, [0,T], nt=nt, stepper="rk4", alph=net.alph)
    return Jc, cs


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    if args.data == 'Keystrokes':
        d = 1
    else:
        d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)
    T      = args.T

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None
    
    savedir = f"../../assets/{args.data}/"
    if not os.path.exists(savedir):
        os.makedirs(savedir)

    # setup data [nSamples, d]
    # use one batch as the entire data set
    if args.data == 'student-t':
        misc_params = {'df':args.df}
    else:
        misc_params = {}

    x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)
    x0 = cvt(torch.from_numpy(x0))

    x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, misc_params=misc_params)
    x0val = cvt(torch.from_numpy(x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s}      {:9s}  {:9s}  {:9s}  {:9s}  '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'valLoss', 'valL', 'valC', 'valR'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs  = compute_loss(net, x0, nt=nt)
        loss.backward()
        optim.step()

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e}  '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs = compute_loss(net, x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss   = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()
                

        logger.info(log_message) # print iteration

        # create plots
        if itr % args.viz_freq == 0:
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                nSamples = 20000
                p_samples = cvt(torch.Tensor( toy_data.inf_train_gen(args.data, batch_size=nSamples, misc_params=misc_params) ))
                y = cvt(torch.randn(nSamples,d)) # sampling from the standard normal (rho_1)

                '''
                if d == 2:
                    sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                    plot4(net, p_samples, y, [0,T], nt_val, sPath, doPaths=True, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                            ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh))
                '''
                # save result
                d = net.d
                fx = integrate(p_samples[:, 0:d], net, [0,T], nt_val, stepper="rk4", alph=net.alph)
                finvfx = integrate(fx[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
                genModel = integrate(y[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
    
                if args.exclude_OT == False:
                    filename = f"{savedir}otflow_"
                else:
                    filename = f"{savedir}no_otflow_"
                if args.df != None:
                    filename += f"df{args.df}_{args.batch_size}samples.pickle"
                else:
                    filename += f"{args.batch_size}samples.pickle"
                
                with open(filename,"wb") as fw:
                    pickle.dump([fx.cpu().numpy()[:, 0:d], genModel.cpu().numpy()[:, 0:d], ] , fw)
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
    logger.info(f"Result was stored in {filename}.")






start time: 2024_05_21_22_06_36
Namespace(data='student-t', df=1.0, nt=16, nt_val=16, alph=[1.0, 100.0, 5.0], m=32, nTh=2, T=10.0, exclude_OT=False, niters=2, batch_size=10000, val_batch_size=4096, lr=0.1, drop_freq=100, weight_decay=0.0, lr_drop=2.0, optim='adam', prec='single', save='experiments/cnf/toy', viz_freq=100, val_freq=1, gpu=0, sample_freq=10000)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=16   nt_val=16
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.1
    maximize: False
    weight_decay: 0.0
)
data=student-t batch_size=10000 gpu=0
maxIters=2 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
00001   1.267   3.774e+04  5.138e+03  6.311e+01  5.259e+03      6.843e+04  7.327e+03  2.356e+02  7.509e+03 
00002   1.298   4.755e+04  3.395e+03  2.625e+02  3.582e+03      5.248e+04  7.582e+03  5.479e+01  7.885e+03 
Training Time: 2.5659492015838623 seconds
Training has finished.  2024_05_21_22_06_36_student-t_alph100_5_m32_checkpt.pth
/Users/hyemin/Documents/source_code/Proximal_generative_models/scripts/ot_flow-master/trainToyOTflow.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import plot4
from src.OTFlowProblem import *
import config
import pickle

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 1500
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings', 'student-t', 'Keystrokes'],
    type=str, default='student-t'
)
parser.add_argument(
    '-df', type=float, help="degree of freedom for student-t distribution"
)

parser.add_argument("--nt"    , type=int, default=16, help="number of time steps")
parser.add_argument("--nt_val", type=int, default=16, help="number of time steps for validation")
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0')
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)
parser.add_argument('--T'     , type=float, default=1.0)
parser.add_argument('--exclude_OT'  , type=bool, default=False)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)


parser.add_argument('--lr'          , type=float, default=0.1)
parser.add_argument("--drop_freq"   , type=int  , default=100, help="how often to decrease learning rate")
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='single', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=10000)#25)


args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]
if args.exclude_OT:
    args.alph[0] = 0.0
    args.alph[2] = 0.0

# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

torch.manual_seed(0)
device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def compute_loss(net, x, nt):
    Jc , cs = OTFlowProblem(x, net, [0,T], nt=nt, stepper="rk4", alph=net.alph)
    return Jc, cs


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    if args.data == 'Keystrokes':
        d = 1
    else:
        d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)
    T      = args.T

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None
    
    savedir = f"../../assets/{args.data}/"
    if not os.path.exists(savedir):
        os.makedirs(savedir)

    # setup data [nSamples, d]
    # use one batch as the entire data set
    if args.data == 'student-t':
        misc_params = {'df':args.df}
    else:
        misc_params = {}

    x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)
    x0 = cvt(torch.from_numpy(x0))

    x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, misc_params=misc_params)
    x0val = cvt(torch.from_numpy(x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s}      {:9s}  {:9s}  {:9s}  {:9s}  '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'valLoss', 'valL', 'valC', 'valR'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs  = compute_loss(net, x0, nt=nt)
        loss.backward()
        optim.step()

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e}  '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs = compute_loss(net, x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss   = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()
                

        logger.info(log_message) # print iteration

        # create plots
        if itr % args.viz_freq == 0:
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                nSamples = 20000
                p_samples = cvt(torch.Tensor( toy_data.inf_train_gen(args.data, batch_size=nSamples, misc_params=misc_params) ))
                y = cvt(torch.randn(nSamples,d)) # sampling from the standard normal (rho_1)

                '''
                if d == 2:
                    sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                    plot4(net, p_samples, y, [0,T], nt_val, sPath, doPaths=True, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                            ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh))
                '''
                # save result
                d = net.d
                fx = integrate(p_samples[:, 0:d], net, [0,T], nt_val, stepper="rk4", alph=net.alph)
                finvfx = integrate(fx[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
                genModel = integrate(y[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
    
                if args.exclude_OT == False:
                    filename = f"{savedir}otflow_"
                else:
                    filename = f"{savedir}no_otflow_"
                if args.df != None:
                    filename += f"df{args.df}_{args.batch_size}samples.pickle"
                else:
                    filename += f"{args.batch_size}samples.pickle"
                
                with open(filename,"wb") as fw:
                    pickle.dump([fx.cpu().numpy()[:, 0:d], genModel.cpu().numpy()[:, 0:d], ] , fw)
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
    logger.info(f"Result was stored in {filename}.")






start time: 2024_05_21_22_06_42
Namespace(data='student-t', df=3.0, nt=16, nt_val=16, alph=[1.0, 100.0, 5.0], m=32, nTh=2, T=10.0, exclude_OT=False, niters=2000, batch_size=10000, val_batch_size=4096, lr=0.1, drop_freq=100, weight_decay=0.0, lr_drop=2.0, optim='adam', prec='single', save='experiments/cnf/toy', viz_freq=100, val_freq=1, gpu=0, sample_freq=10000)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=16   nt_val=16
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.1
    maximize: False
    weight_decay: 0.0
)
data=student-t batch_size=10000 gpu=0
maxIters=2000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
/Users/hyemin/Documents/source_code/Proximal_generative_models/scripts/ot_flow-master/trainToyOTflow.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import plot4
from src.OTFlowProblem import *
import config
import pickle

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 2
    def_batch    = 4096
    def_niter    = 1500
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings', 'student-t', 'Keystrokes'],
    type=str, default='student-t'
)
parser.add_argument(
    '-df', type=float, help="degree of freedom for student-t distribution"
)

parser.add_argument("--nt"    , type=int, default=16, help="number of time steps")
parser.add_argument("--nt_val", type=int, default=16, help="number of time steps for validation")
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0')
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)
parser.add_argument('--T'     , type=float, default=1.0)
parser.add_argument('--exclude_OT'  , type=bool, default=False)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)


parser.add_argument('--lr'          , type=float, default=0.1)
parser.add_argument("--drop_freq"   , type=int  , default=100, help="how often to decrease learning rate")
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='single', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=10000)#25)


args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]
if args.exclude_OT:
    args.alph[0] = 0.0
    args.alph[2] = 0.0

# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

torch.manual_seed(0)
device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def compute_loss(net, x, nt):
    Jc , cs = OTFlowProblem(x, net, [0,T], nt=nt, stepper="rk4", alph=net.alph)
    return Jc, cs


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    if args.data == 'Keystrokes':
        d = 1
    else:
        d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)
    T      = args.T

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None
    
    savedir = f"../../assets/{args.data}/"
    if not os.path.exists(savedir):
        os.makedirs(savedir)

    # setup data [nSamples, d]
    # use one batch as the entire data set
    if args.data == 'student-t':
        misc_params = {'df':args.df}
    else:
        misc_params = {}

    x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)
    x0 = cvt(torch.from_numpy(x0))

    x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, misc_params=misc_params)
    x0val = cvt(torch.from_numpy(x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s}      {:9s}  {:9s}  {:9s}  {:9s}  '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'valLoss', 'valL', 'valC', 'valR'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs  = compute_loss(net, x0, nt=nt)
        loss.backward()
        optim.step()

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e}  '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs = compute_loss(net, x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss   = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()
                

        logger.info(log_message) # print iteration

        # create plots
        if itr % args.viz_freq == 0:
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                nSamples = 20000
                p_samples = cvt(torch.Tensor( toy_data.inf_train_gen(args.data, batch_size=nSamples, misc_params=misc_params) ))
                y = cvt(torch.randn(nSamples,d)) # sampling from the standard normal (rho_1)

                '''
                if d == 2:
                    sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                    plot4(net, p_samples, y, [0,T], nt_val, sPath, doPaths=True, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                            ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh))
                '''
                # save result
                d = net.d
                fx = integrate(p_samples[:, 0:d], net, [0,T], nt_val, stepper="rk4", alph=net.alph)
                finvfx = integrate(fx[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
                genModel = integrate(y[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
    
                if args.exclude_OT == False:
                    filename = f"{savedir}otflow_"
                else:
                    filename = f"{savedir}no_otflow_"
                if args.df != None:
                    filename += f"df{args.df}_{args.batch_size}samples.pickle"
                else:
                    filename += f"{args.batch_size}samples.pickle"
                
                with open(filename,"wb") as fw:
                    pickle.dump([fx.cpu().numpy()[:, 0:d], genModel.cpu().numpy()[:, 0:d], ] , fw)
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
    logger.info(f"Result was stored in {filename}.")






start time: 2024_05_21_22_07_27
Namespace(data='student-t', df=1.0, nt=16, nt_val=16, alph=[1.0, 100.0, 5.0], m=32, nTh=2, T=10.0, exclude_OT=False, niters=2, batch_size=10000, val_batch_size=4096, lr=0.1, drop_freq=100, weight_decay=0.0, lr_drop=2.0, optim='adam', prec='single', save='experiments/cnf/toy', viz_freq=2, val_freq=1, gpu=0, sample_freq=10000)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=16   nt_val=16
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.1
    maximize: False
    weight_decay: 0.0
)
data=student-t batch_size=10000 gpu=0
maxIters=2 val_freq=1 viz_freq=2
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
00001   1.297   3.774e+04  5.138e+03  6.311e+01  5.259e+03      6.843e+04  7.327e+03  2.356e+02  7.509e+03 
00002   1.311   4.755e+04  3.395e+03  2.625e+02  3.582e+03      5.248e+04  7.582e+03  5.479e+01  7.885e+03 
Training Time: 2.608201265335083 seconds
Training has finished.  2024_05_21_22_07_27_student-t_alph100_5_m32_checkpt.pth
Result was stored in ../../assets/student-t/otflow_df1.0_10000samples.pickle.
/Users/hyemin/Documents/source_code/Proximal_generative_models/scripts/ot_flow-master/trainToyOTflow.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import plot4
from src.OTFlowProblem import *
import config
import pickle

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 2
    def_batch    = 4096
    def_niter    = 1500
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings', 'student-t', 'Keystrokes'],
    type=str, default='student-t'
)
parser.add_argument(
    '-df', type=float, help="degree of freedom for student-t distribution"
)

parser.add_argument("--nt"    , type=int, default=16, help="number of time steps")
parser.add_argument("--nt_val", type=int, default=16, help="number of time steps for validation")
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0')
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)
parser.add_argument('--T'     , type=float, default=1.0)
parser.add_argument('--exclude_OT'  , type=bool, default=False)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)


parser.add_argument('--lr'          , type=float, default=0.1)
parser.add_argument("--drop_freq"   , type=int  , default=100, help="how often to decrease learning rate")
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='single', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=10000)#25)


args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]
if args.exclude_OT:
    args.alph[0] = 0.0
    args.alph[2] = 0.0

# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

torch.manual_seed(0)
device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def compute_loss(net, x, nt):
    Jc , cs = OTFlowProblem(x, net, [0,T], nt=nt, stepper="rk4", alph=net.alph)
    return Jc, cs


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    if args.data == 'Keystrokes':
        d = 1
    else:
        d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)
    T      = args.T

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None
    
    savedir = f"../../assets/{args.data}/"
    if not os.path.exists(savedir):
        os.makedirs(savedir)

    # setup data [nSamples, d]
    # use one batch as the entire data set
    if args.data == 'student-t':
        misc_params = {'df':args.df}
    else:
        misc_params = {}

    x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)
    x0 = cvt(torch.from_numpy(x0))

    x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, misc_params=misc_params)
    x0val = cvt(torch.from_numpy(x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s}      {:9s}  {:9s}  {:9s}  {:9s}  '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'valLoss', 'valL', 'valC', 'valR'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs  = compute_loss(net, x0, nt=nt)
        loss.backward()
        optim.step()

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e}  '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs = compute_loss(net, x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss   = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()
                

        logger.info(log_message) # print iteration

        # create plots
        if itr % args.viz_freq == 0:
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                nSamples = 20000
                p_samples = cvt(torch.Tensor( toy_data.inf_train_gen(args.data, batch_size=nSamples, misc_params=misc_params) ))
                y = cvt(torch.randn(nSamples,d)) # sampling from the standard normal (rho_1)

                '''
                if d == 2:
                    sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                    plot4(net, p_samples, y, [0,T], nt_val, sPath, doPaths=True, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                            ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh))
                '''
                # save result
                d = net.d
                fx = integrate(p_samples[:, 0:d], net, [0,T], nt_val, stepper="rk4", alph=net.alph)
                finvfx = integrate(fx[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
                genModel = integrate(y[:, 0:d], net, [T,0], nt_val, stepper="rk4", alph=net.alph)
    
                if args.exclude_OT == False:
                    filename = f"{savedir}otflow_"
                else:
                    filename = f"{savedir}no_otflow_"
                if args.df != None:
                    filename += f"df{args.df}_{args.batch_size}samples.pickle"
                else:
                    filename += f"{args.batch_size}samples.pickle"
                
                with open(filename,"wb") as fw:
                    pickle.dump([fx.cpu().numpy()[:, 0:d], genModel.cpu().numpy()[:, 0:d], ] , fw)
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, misc_params=misc_params)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
    logger.info(f"Result was stored in {filename}.")






start time: 2024_05_21_22_07_37
Namespace(data='student-t', df=3.0, nt=16, nt_val=16, alph=[1.0, 100.0, 5.0], m=32, nTh=2, T=10.0, exclude_OT=False, niters=2000, batch_size=10000, val_batch_size=4096, lr=0.1, drop_freq=100, weight_decay=0.0, lr_drop=2.0, optim='adam', prec='single', save='experiments/cnf/toy', viz_freq=2, val_freq=1, gpu=0, sample_freq=10000)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=16   nt_val=16
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.1
    maximize: False
    weight_decay: 0.0
)
data=student-t batch_size=10000 gpu=0
maxIters=2000 val_freq=1 viz_freq=2
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
